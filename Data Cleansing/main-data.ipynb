{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date Culomn: 1\n",
    "Time Culomn: 2\n",
    "CO Culomn: 4\n",
    "SO2 Culomn: 8\n",
    "PM10 Culomn: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) شادآباد.منطقه18.xlsx 17442\n",
      "2 ) ستاد.بحران.منطقه7.xlsx 17393\n",
      "3 ) پونک.xlsx 17486\n",
      "4 ) پیروزی.منطقه13.xlsx 16953\n",
      "5 ) میدان.فتح.منطقه9.xlsx 17184\n",
      "6 ) شهرداری.منطقه21.xlsx 17389\n",
      "7 ) شهرداری.منطقه22.xlsx 17145\n",
      "8 ) اتوبان.محلاتی.منطقه14.xlsx 17073\n",
      "9 ) تربیت.مدرس.منطقه6.xlsx 17356\n",
      "10 ) شهرداری.منطقه.2.xlsx 17457\n",
      "11 ) اقدسیه.منطقه1.xlsx 17435\n",
      "12 ) مسعودیه.منطقه15.xlsx 17226\n",
      "13 ) شهر.ری.منطقه20.xlsx 17265\n"
     ]
    }
   ],
   "source": [
    "# Set the directory containing the Excel files\n",
    "directory = 'Dataset/'\n",
    "\n",
    "# Create an empty list to store the dataframes\n",
    "dfs = []\n",
    "index = 0\n",
    "# Loop through each file in the directory from seocnd row\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Read the data from the Excel file\n",
    "        data = pd.read_excel(os.path.join(directory, filename))\n",
    "\n",
    "        # cast to float\n",
    "        data.iloc[1:, [4,8,9]] = data.iloc[1:, [4,8,9]].astype(float)\n",
    "\n",
    "\n",
    "        # add index to column 0\n",
    "        data.iloc[0] = [index for j in range(data.shape[1])]\n",
    "        index += 1\n",
    "\n",
    "        print(index, ')',filename, data.shape[0])\n",
    "\n",
    "        # Add the dataframe from second row to the list\n",
    "        # dfs.append(data.iloc[2:, [0,1,4,8,9]])\n",
    "        dfs.append(pd.DataFrame(data={'date': data.iloc[1:, 1],\n",
    "                    'time': data.iloc[1:, 2],\n",
    "                    'co': data.iloc[1:, 4],\n",
    "                    'so2': data.iloc[1:, 8],\n",
    "                    'pm10': data.iloc[1:, 9]}, index = [i for i in range(data.shape[0] - 1)]))\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date time        co       so2       pm10\n",
      "1      1400/01/01    0  1.187312    3.2108       45.5\n",
      "2      1400/01/01    2  1.371816  3.274771  47.399814\n",
      "3      1400/01/01    3  1.021523  3.434897  36.807692\n",
      "4      1400/01/01    4  1.139674  3.843559  20.463772\n",
      "5      1400/01/01    5  1.109617  3.235971  32.748563\n",
      "...           ...  ...       ...       ...        ...\n",
      "17436  1401/12/29   18       NaN  2.596993  20.140805\n",
      "17437  1401/12/29   19       NaN  3.119621  53.106667\n",
      "17438  1401/12/29   20       NaN  5.027132        NaN\n",
      "17439  1401/12/29   21       NaN  5.325556        NaN\n",
      "17440  1401/12/29   22       NaN  5.794776  47.086207\n",
      "\n",
      "[17440 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfs[0])\n",
    "# print(dfs[0][dfs[0][1] == '1400/01/01' & dfs[0][2] == '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# date\n",
    "date = '1401/01/04'\n",
    "hour = 0\n",
    "i = 2\n",
    "\n",
    "print(dfs[i][(dfs[i]['date'] == date) & (dfs[i]['time'] == hour)].shape[0] > 0)\n",
    "\n",
    "# # check if date and hour exist in dfs[i] in same row\n",
    "# if dfs[i][dfs[i].date == date & dfs[i].time == hour].shape[0] > 0:\n",
    "#     print('yes')\n",
    "#     # get index\n",
    "#     index = dfs[1].index[(dfs[i].iloc[:, 1] == date) & (dfs[i].iloc[:, 2] == hour)].tolist()[0]\n",
    "#     print(index)\n",
    "#     print(dfs[i].iloc[index - 1])\n",
    "#     # dfs[1].drop(index, inplace=True)\n",
    "#     # print(dfs[1].iloc[index - 1])\n",
    "# else:\n",
    "#     print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'time', 'co', 'so2', 'pm10'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hasti/University/Final Project/My Code/Data Cleansing/main-data.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m row\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# get index\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# print('yes', cur_date, cur_hour, i)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     new_row \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m: cur_date, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m: cur_hour, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mco\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39m\u001b[39mco\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mso2\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39m\u001b[39mso2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mpm10\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39m\u001b[39mpm10\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     new_dfs[i]\u001b[39m.\u001b[39;49mloc[\u001b[39mlen\u001b[39;49m(new_dfs[i])] \u001b[39m=\u001b[39m new_row\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# add null row\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     new_dfs[i]\u001b[39m.\u001b[39mloc[\u001b[39mlen\u001b[39m(new_dfs[i])] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m: cur_date,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                                        \u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m: cur_hour,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                                        \u001b[39m'\u001b[39m\u001b[39mco\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mNaN,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                                        \u001b[39m'\u001b[39m\u001b[39mso2\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mNaN,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hasti/University/Final%20Project/My%20Code/Data%20Cleansing/main-data.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                                        \u001b[39m'\u001b[39m\u001b[39mpm10\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mNaN}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexing.py:723\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    722\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[0;32m--> 723\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexing.py:1724\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     indexer, missing \u001b[39m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[1;32m   1723\u001b[0m     \u001b[39mif\u001b[39;00m missing:\n\u001b[0;32m-> 1724\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_missing(indexer, value)\n\u001b[1;32m   1725\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexing.py:2031\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2027\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot set a row with mismatched columns\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2029\u001b[0m     value \u001b[39m=\u001b[39m Series(value, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mcolumns, name\u001b[39m=\u001b[39mindexer)\n\u001b[0;32m-> 2031\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mappend(value)\u001b[39m.\u001b[39m_mgr\n\u001b[1;32m   2032\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_maybe_update_cacher(clear\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py:8965\u001b[0m, in \u001b[0;36mDataFrame.append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   8962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   8963\u001b[0m     to_concat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m, other]\n\u001b[1;32m   8964\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m-> 8965\u001b[0m     concat(\n\u001b[1;32m   8966\u001b[0m         to_concat,\n\u001b[1;32m   8967\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m   8968\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m   8969\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   8970\u001b[0m     )\n\u001b[1;32m   8971\u001b[0m )\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:307\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39malong the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    295\u001b[0m     objs,\n\u001b[1;32m    296\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m    305\u001b[0m )\n\u001b[0;32m--> 307\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/reshape/concat.py:532\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    530\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 532\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[1;32m    533\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[1;32m    534\u001b[0m )\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy:\n\u001b[1;32m    536\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/internals/concat.py:226\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    224\u001b[0m     fastpath \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m values\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     values \u001b[39m=\u001b[39m _concatenate_join_units(join_units, concat_axis, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    227\u001b[0m     fastpath \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m fastpath:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/internals/concat.py:523\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    520\u001b[0m     concat_values \u001b[39m=\u001b[39m ensure_block_shape(concat_values, \u001b[39m2\u001b[39m)\n\u001b[1;32m    522\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m     concat_values \u001b[39m=\u001b[39m concat_compat(to_concat, axis\u001b[39m=\u001b[39;49mconcat_axis)\n\u001b[1;32m    525\u001b[0m \u001b[39mreturn\u001b[39;00m concat_values\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/dtypes/concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[0;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m             \u001b[39m# coerce to object\u001b[39;00m\n\u001b[1;32m    149\u001b[0m             to_concat \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[0;32m--> 151\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mconcatenate(to_concat, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cleaning the data\n",
    "years = [1400, 1401]\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "days = [31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 29]\n",
    "hours = [i for i in range(24)]\n",
    "\n",
    "prv_date = '1400/01/01'\n",
    "prv_time = 0\n",
    "column_names = dfs[0].columns\n",
    "print(column_names)\n",
    "new_dfs = [pd.DataFrame(columns=column_names) for i in range(len(dfs))]\n",
    "\n",
    "for year in years: \n",
    "    for max_day, month in zip(days, months):\n",
    "        for day in range(1, max_day + 1):\n",
    "            if day < 10:\n",
    "                day = f'0{day}'\n",
    "            cur_date = f'{year}/{month}/{day}'\n",
    "            for cur_hour in hours:\n",
    "                for i in range(len(dfs)):\n",
    "                    row = dfs[i][(dfs[i]['date'] == cur_date) & (dfs[i]['time'] == cur_hour)] \n",
    "                    if row.shape[0] > 0:\n",
    "                        # get index\n",
    "                        # print('yes', cur_date, cur_hour, i)\n",
    "                        new_row = {'date': cur_date, \n",
    "                                   'time': cur_hour, \n",
    "                                   'co': row['co'].values[0], \n",
    "                                   'so2': row['so2'].values[0], \n",
    "                                   'pm10': row['pm10'].values[0]}\n",
    "                        new_dfs[i].loc[len(new_dfs[i])] = new_row\n",
    "                        \n",
    "                    else:\n",
    "                        # add null row\n",
    "                        new_dfs[i].loc[len(new_dfs[i])] = {'date': cur_date,\n",
    "                                                           'time': cur_hour,\n",
    "                                                           'co': np.NaN,\n",
    "                                                           'so2': np.NaN,\n",
    "                                                           'pm10': np.NaN}\n",
    "                        \n",
    "                prv_time = cur_hour\n",
    "            prv_date = cur_date\n",
    "            # print(new_dfs[0])\n",
    "        print(f'{year}/{month} done')\n",
    "            \n",
    "print(new_dfs[0])\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "for i in range(len(new_dfs)):\n",
    "    new_dfs[i].to_csv(f'Clean with NaN/cleaned_data_{i}.csv', index=False)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv\n",
    "new_dfs = []\n",
    "for i in range(len(dfs)):\n",
    "    new_dfs.append(pd.read_csv(f'Clean with NaN/cleaned_data_{i}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n",
      "(17520, 5)\n"
     ]
    }
   ],
   "source": [
    "# print shape of each dataframe\n",
    "for i in range(len(new_dfs)):\n",
    "    print(new_dfs[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows done\n",
      "500 rows done\n",
      "1000 rows done\n",
      "1500 rows done\n",
      "2000 rows done\n",
      "2500 rows done\n",
      "3000 rows done\n",
      "3500 rows done\n",
      "4000 rows done\n",
      "4500 rows done\n",
      "5000 rows done\n",
      "5500 rows done\n",
      "6000 rows done\n",
      "6500 rows done\n",
      "7000 rows done\n",
      "7500 rows done\n",
      "8000 rows done\n",
      "8500 rows done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h8/mhwh4fjn0qj405sjdd9_5ffw0000gn/T/ipykernel_22646/1167662963.py:20: RuntimeWarning: Mean of empty slice\n",
      "  co = np.nanmean([all_rows[k][2] for k in range(node_num)])\n",
      "/var/folders/h8/mhwh4fjn0qj405sjdd9_5ffw0000gn/T/ipykernel_22646/1167662963.py:21: RuntimeWarning: Mean of empty slice\n",
      "  so2 = np.nanmean([all_rows[k][3] for k in range(node_num)])\n",
      "/var/folders/h8/mhwh4fjn0qj405sjdd9_5ffw0000gn/T/ipykernel_22646/1167662963.py:22: RuntimeWarning: Mean of empty slice\n",
      "  pm10 = np.nanmean([all_rows[k][4] for k in range(node_num)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 rows done\n",
      "9500 rows done\n",
      "10000 rows done\n",
      "10500 rows done\n",
      "11000 rows done\n",
      "11500 rows done\n",
      "12000 rows done\n",
      "12500 rows done\n",
      "13000 rows done\n",
      "13500 rows done\n",
      "14000 rows done\n",
      "14500 rows done\n",
      "15000 rows done\n",
      "15500 rows done\n",
      "16000 rows done\n",
      "16500 rows done\n",
      "17000 rows done\n",
      "17500 rows done\n",
      "             date  time        co       so2       pm10\n",
      "0      1400/01/01     0  1.187312  3.210800  45.500000\n",
      "1      1400/01/01     1  1.187312  3.210800  45.500000\n",
      "2      1400/01/01     2  1.371816  3.274771  47.399814\n",
      "3      1400/01/01     3  1.021523  3.434897  36.807692\n",
      "4      1400/01/01     4  1.139674  3.843559  20.463772\n",
      "...           ...   ...       ...       ...        ...\n",
      "17515  1401/12/29    19  0.954321  3.119621  53.106667\n",
      "17516  1401/12/29    20  1.258243  5.027132  37.154505\n",
      "17517  1401/12/29    21  1.410335  5.325556  37.341226\n",
      "17518  1401/12/29    22  1.508488  5.794776  47.086207\n",
      "17519  1401/12/29    23  1.508488  5.794776  47.086207\n",
      "\n",
      "[17520 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# for null values: make average of other values in same day and hour\n",
    "\n",
    "row_num = new_dfs[0].shape[0]\n",
    "col_num = new_dfs[0].shape[1]\n",
    "node_num = len(new_dfs)\n",
    "\n",
    "# print(row_num, node_num)\n",
    "\n",
    "# iterate on rows of every dataframe\n",
    "for i in range(row_num):\n",
    "    all_rows = []\n",
    "\n",
    "    # iterate on dataframes\n",
    "    for j in range(node_num):\n",
    "        all_rows.append(new_dfs[j].iloc[i].to_list())\n",
    "    \n",
    "    # print(f'{i}) ** ', all_rows)\n",
    "\n",
    "    co = np.nanmean([all_rows[k][2] for k in range(node_num)])\n",
    "    so2 = np.nanmean([all_rows[k][3] for k in range(node_num)])\n",
    "    pm10 = np.nanmean([all_rows[k][4] for k in range(node_num)])\n",
    "\n",
    "    \n",
    "    # print(f'{i})', co, so2, pm10)\n",
    "\n",
    "    # fill the null values with 0.7 * average + 0.3 * previous value\n",
    "    for j in range(node_num):\n",
    "        # co2 \n",
    "        if np.isnan(all_rows[j][2]):\n",
    "            if np.isnan(co):\n",
    "                new_dfs[j].iloc[i, 2] = new_dfs[j].iloc[i - 1, 2]\n",
    "            else:\n",
    "                new_dfs[j].iloc[i, 2] = 0.7 * co + 0.3 * new_dfs[j].iloc[i - 1, 2]\n",
    "\n",
    "        # so2\n",
    "        if np.isnan(all_rows[j][3]):\n",
    "            if np.isnan(so2):\n",
    "                new_dfs[j].iloc[i, 3] = new_dfs[j].iloc[i - 1, 3]\n",
    "            else:\n",
    "                new_dfs[j].iloc[i, 3] = 0.7 * so2 + 0.3 * new_dfs[j].iloc[i - 1, 3]\n",
    "\n",
    "        # pm10\n",
    "        if np.isnan(all_rows[j][4]):\n",
    "            if np.isnan(pm10):\n",
    "                new_dfs[j].iloc[i, 4] = new_dfs[j].iloc[i - 1, 4]\n",
    "            else:\n",
    "                new_dfs[j].iloc[i, 4] = 0.7 * pm10 + 0.3 * new_dfs[j].iloc[i - 1, 4]\n",
    "        \n",
    "    # if i == 3:\n",
    "    #     break\n",
    "    if i % 500 == 0:\n",
    "        print(f'{i} rows done')\n",
    "\n",
    "print(new_dfs[0])\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "for i in range(len(new_dfs)):\n",
    "\n",
    "    new_dfs[i].to_csv(f'Clean/cleaned_data_{i}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
