{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date Culomn: 1\n",
    "\n",
    "Time Culomn: 2\n",
    "\n",
    "CO Culomn: 4\n",
    "\n",
    "SO2 Culomn: 8\n",
    "\n",
    "PM10 Culomn: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET Parameters \n",
    "\n",
    "# DAILY\n",
    "co_col = 3\n",
    "so2_col = 7\n",
    "pm10_col = 8\n",
    "\n",
    "# HOURLY\n",
    "# co_col = 4\n",
    "# so2_col = 8\n",
    "# pm10_col = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) اتوبان.محلاتی.منطقه14 (1).xlsx 1053\n",
      "2 ) ستاد.بحران.منطقه7.xlsx 1093\n",
      "3 ) پونک.xlsx 1096\n",
      "4 ) پیروزی.منطقه13.xlsx 1075\n",
      "5 ) میدان.فتح.منطقه9.xlsx 1091\n",
      "6 ) گلبرگ.منطقه8.xlsx 1065\n",
      "7 ) شهرداری.منطقه22.xlsx 1087\n",
      "8 ) شهرداری.منطقه19.xlsx 1026\n",
      "9 ) تربیت.مدرس.منطقه6.xlsx 1080\n",
      "10 ) شهرداری.منطقه.2.xlsx 1096\n",
      "11 ) اقدسیه.منطقه1.xlsx 1095\n"
     ]
    }
   ],
   "source": [
    "# Set the directory containing the Excel files\n",
    "directory = 'Dataset/'\n",
    "\n",
    "# Create an empty list to store the dataframes\n",
    "dfs = []\n",
    "index = 0\n",
    "\n",
    "\n",
    "# Loop through each file in the directory from seocnd row\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Read the data from the Excel file\n",
    "        data = pd.read_excel(os.path.join(directory, filename))\n",
    "\n",
    "        # cast to float\n",
    "        data.iloc[1:, [co_col, so2_col, pm10_col]] = data.iloc[1:, [co_col, so2_col, pm10_col]].astype(float)\n",
    "\n",
    "\n",
    "        # add index to column 0\n",
    "        data.iloc[0] = [index for j in range(data.shape[1])]\n",
    "        index += 1\n",
    "\n",
    "        print(index, ')',filename, data.shape[0])\n",
    "\n",
    "        # Add the dataframe from second row to the list\n",
    "        # dfs.append(data.iloc[2:, [0,1,4,8,9]])\n",
    "        dfs.append(pd.DataFrame(data={'date': data.iloc[1:, 1],\n",
    "                    # 'time': data.iloc[1:, 2],\n",
    "                    'co': data.iloc[1:, co_col],\n",
    "                    'so2': data.iloc[1:, so2_col],\n",
    "                    'pm10': data.iloc[1:, pm10_col]}, index = [i for i in range(data.shape[0])]))\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date        co       so2       pm10\n",
      "1     1399/01/01  1.432762  3.100977  25.754943\n",
      "2     1399/01/02  1.134112  2.991404  26.339174\n",
      "3     1399/01/03  0.720623  3.325601  26.231035\n",
      "4     1399/01/04  0.784516  2.828343  12.636616\n",
      "5     1399/01/05  1.107031  3.673456  18.629067\n",
      "...          ...       ...       ...        ...\n",
      "1088  1401/12/25       NaN  3.069358        NaN\n",
      "1089  1401/12/26       NaN  3.494713        NaN\n",
      "1090  1401/12/27       NaN  1.931176        NaN\n",
      "1091  1401/12/28       NaN  2.164131        NaN\n",
      "1092  1401/12/29       NaN  2.833674        NaN\n",
      "\n",
      "[1092 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfs[1])\n",
    "# print(dfs[0][dfs[0][1] == '1400/01/01' & dfs[0][2] == '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'co', 'so2', 'pm10'], dtype='object')\n",
      "1399/01 done\n",
      "1399/02 done\n",
      "1399/03 done\n",
      "1399/04 done\n",
      "1399/05 done\n",
      "1399/06 done\n",
      "1399/07 done\n",
      "1399/08 done\n",
      "1399/09 done\n",
      "1399/10 done\n",
      "1399/11 done\n",
      "1399/12 done\n",
      "1400/01 done\n",
      "1400/02 done\n",
      "1400/03 done\n",
      "1400/04 done\n",
      "1400/05 done\n",
      "1400/06 done\n",
      "1400/07 done\n",
      "1400/08 done\n",
      "1400/09 done\n",
      "1400/10 done\n",
      "1400/11 done\n",
      "1400/12 done\n",
      "1401/01 done\n",
      "1401/02 done\n",
      "1401/03 done\n",
      "1401/04 done\n",
      "1401/05 done\n",
      "1401/06 done\n",
      "1401/07 done\n",
      "1401/08 done\n",
      "1401/09 done\n",
      "1401/10 done\n",
      "1401/11 done\n",
      "1401/12 done\n",
      "            date        co       so2       pm10\n",
      "0     1399/01/01  0.781111  2.879632  34.937691\n",
      "1     1399/01/02  0.483451  2.979898  32.153485\n",
      "2     1399/01/03  0.485140  3.565474  32.807312\n",
      "3     1399/01/04  0.634489  2.769805  15.075615\n",
      "4     1399/01/05  0.861081  5.952307  26.082708\n",
      "...          ...       ...       ...        ...\n",
      "1090  1401/12/25  1.245443  3.299031  33.972426\n",
      "1091  1401/12/26  1.137636  3.895407  39.107264\n",
      "1092  1401/12/27  1.398713  4.218286  51.343667\n",
      "1093  1401/12/28  1.950093  6.368767  54.159222\n",
      "1094  1401/12/29  1.019187  2.957621  33.714621\n",
      "\n",
      "[1095 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data <DAILY>\n",
    "years = [1399, 1400, 1401]\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "days = [31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 29]\n",
    "\n",
    "prv_date = '1400/01/01'\n",
    "\n",
    "column_names = dfs[0].columns\n",
    "print(column_names)\n",
    "new_dfs = [pd.DataFrame(columns=column_names) for i in range(len(dfs))]\n",
    "\n",
    "null_counter = [0 for i in range(len(dfs))]\n",
    "\n",
    "for year in years: \n",
    "    for max_day, month in zip(days, months):\n",
    "        for day in range(1, max_day + 1):\n",
    "            if day < 10:\n",
    "                day = f'0{day}'\n",
    "            cur_date = f'{year}/{month}/{day}'\n",
    "            for i in range(len(dfs)):\n",
    "                row = dfs[i][dfs[i]['date'] == cur_date] \n",
    "                if row.shape[0] > 0:\n",
    "                    # get index\n",
    "                    # print('yes', cur_date, cur_hour, i)\n",
    "                    new_row = {'date': cur_date, \n",
    "                            'co': row['co'].values[0], \n",
    "                            'so2': row['so2'].values[0], \n",
    "                            'pm10': row['pm10'].values[0]}\n",
    "                    new_dfs[i].loc[len(new_dfs[i])] = new_row\n",
    "                    \n",
    "                else:\n",
    "                    # add null row\n",
    "                    new_dfs[i].loc[len(new_dfs[i])] = {'date': cur_date,\n",
    "                                                    'co': np.NaN,\n",
    "                                                    'so2': np.NaN,\n",
    "                                                    'pm10': np.NaN}\n",
    "                                                           \n",
    "                    null_counter[i] += 1\n",
    "            prv_date = cur_date\n",
    "            # print(new_dfs[0])\n",
    "        print(f'{year}/{month} done')\n",
    "            \n",
    "print(new_dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emptying directory\n",
    "for filename in os.listdir('Clean with NaN/'):\n",
    "    if filename.endswith('.csv'):\n",
    "        os.remove(os.path.join('Clean with NaN/', filename))\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "for i in range(len(new_dfs)):\n",
    "    new_dfs[i].to_csv(f'Clean with NaN/cleaned_data_{i}.csv', index=False)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 44\n",
      "1 : 4\n",
      "2 : 1\n",
      "3 : 22\n",
      "4 : 6\n",
      "5 : 32\n",
      "6 : 10\n",
      "7 : 71\n",
      "8 : 17\n",
      "9 : 1\n",
      "10 : 2\n"
     ]
    }
   ],
   "source": [
    "# NULL counter \n",
    "for i in range(len(dfs)):\n",
    "    print(f'{i} : {null_counter[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning the data <HOURLY>\n",
    "# years = [1400, 1401]\n",
    "# months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "# days = [31, 31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 29]\n",
    "# hours = [i for i in range(24)]\n",
    "\n",
    "# prv_date = '1400/01/01'\n",
    "# prv_time = 0\n",
    "# column_names = dfs[0].columns\n",
    "# print(column_names)\n",
    "# new_dfs = [pd.DataFrame(columns=column_names) for i in range(len(dfs))]\n",
    "\n",
    "# null_counter = [0 for i in range(len(dfs))]\n",
    "\n",
    "# for year in years: \n",
    "#     for max_day, month in zip(days, months):\n",
    "#         for day in range(1, max_day + 1):\n",
    "#             if day < 10:\n",
    "#                 day = f'0{day}'\n",
    "#             cur_date = f'{year}/{month}/{day}'\n",
    "#             for cur_hour in hours:\n",
    "#                 for i in range(len(dfs)):\n",
    "#                     row = dfs[i][(dfs[i]['date'] == cur_date) & (dfs[i]['time'] == cur_hour)] \n",
    "#                     if row.shape[0] > 0:\n",
    "#                         # get index\n",
    "#                         # print('yes', cur_date, cur_hour, i)\n",
    "#                         new_row = {'date': cur_date, \n",
    "#                                    'time': cur_hour, \n",
    "#                                    'co': row['co'].values[0], \n",
    "#                                    'so2': row['so2'].values[0], \n",
    "#                                    'pm10': row['pm10'].values[0]}\n",
    "#                         new_dfs[i].loc[len(new_dfs[i])] = new_row\n",
    "                        \n",
    "#                     else:\n",
    "#                         # add null row\n",
    "#                         new_dfs[i].loc[len(new_dfs[i])] = {'date': cur_date,\n",
    "#                                                            'time': cur_hour,\n",
    "#                                                            'co': np.NaN,\n",
    "#                                                            'so2': np.NaN,\n",
    "#                                                            'pm10': np.NaN}\n",
    "                                                           \n",
    "                        \n",
    "#                 prv_time = cur_hour\n",
    "#             prv_date = cur_date\n",
    "#             # print(new_dfs[0])\n",
    "#         print(f'{year}/{month} done')\n",
    "            \n",
    "# print(new_dfs[0])\n",
    "\n",
    "# # Save the dataframes to csv files\n",
    "# for i in range(len(new_dfs)):\n",
    "#     new_dfs[i].to_csv(f'Clean with NaN/cleaned_data_{i}.csv', index=False)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_data_10.csv\n",
      "cleaned_data_1.csv\n",
      "cleaned_data_0.csv\n",
      "cleaned_data_2.csv\n",
      "cleaned_data_3.csv\n",
      "cleaned_data_7.csv\n",
      "cleaned_data_6.csv\n",
      "cleaned_data_4.csv\n",
      "cleaned_data_5.csv\n",
      "cleaned_data_8.csv\n",
      "cleaned_data_9.csv\n"
     ]
    }
   ],
   "source": [
    "directory = 'Clean with NaN/'\n",
    "# read from csv\n",
    "new_dfs = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith(\".csv\"):\n",
    "        new_dfs.append(pd.read_csv(directory + file))\n",
    "        \n",
    "        # print file name\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n",
      "(1095, 4)\n"
     ]
    }
   ],
   "source": [
    "# print shape of each dataframe\n",
    "for i in range(len(new_dfs)):\n",
    "    print(new_dfs[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095 11\n",
      "0 rows done\n",
      "500 rows done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h8/mhwh4fjn0qj405sjdd9_5ffw0000gn/T/ipykernel_80203/1212951761.py:26: RuntimeWarning: Mean of empty slice\n",
      "  co = np.nanmean([all_rows[k][co_col] for k in range(node_num)])\n",
      "/var/folders/h8/mhwh4fjn0qj405sjdd9_5ffw0000gn/T/ipykernel_80203/1212951761.py:27: RuntimeWarning: Mean of empty slice\n",
      "  so2 = np.nanmean([all_rows[k][so2_col] for k in range(node_num)])\n",
      "/var/folders/h8/mhwh4fjn0qj405sjdd9_5ffw0000gn/T/ipykernel_80203/1212951761.py:28: RuntimeWarning: Mean of empty slice\n",
      "  pm10 = np.nanmean([all_rows[k][pm10_col] for k in range(node_num)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 rows done\n",
      "            date        co       so2       pm10\n",
      "0     1399/01/01  1.287979  1.761700  20.560321\n",
      "1     1399/01/02  0.735360  2.617922  21.318358\n",
      "2     1399/01/03  0.666496  1.939312  19.224940\n",
      "3     1399/01/04  1.087427  2.246175  11.412533\n",
      "4     1399/01/05  1.496053  2.579158  18.692815\n",
      "...          ...       ...       ...        ...\n",
      "1090  1401/12/25  0.991112  3.445038  33.391259\n",
      "1091  1401/12/26  1.007853  3.218236  48.858406\n",
      "1092  1401/12/27  2.292972  4.125232  44.985591\n",
      "1093  1401/12/28  0.916010  3.863412  40.893744\n",
      "1094  1401/12/29  0.733984  3.481937  31.758359\n",
      "\n",
      "[1095 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# for null values: make average of other values in same day and hour\n",
    "\n",
    "row_num = new_dfs[0].shape[0]\n",
    "col_num = new_dfs[0].shape[1]\n",
    "node_num = len(new_dfs)\n",
    "\n",
    "print(row_num, node_num)\n",
    "\n",
    "null_counter = [[0, 0, 0] for i in range(node_num)]\n",
    "\n",
    "# iterate on rows of every dataframe\n",
    "for i in range(row_num):\n",
    "    all_rows = []\n",
    "\n",
    "    # iterate on dataframes\n",
    "    for j in range(node_num):\n",
    "        all_rows.append(new_dfs[j].iloc[i].to_list())\n",
    "    \n",
    "    # print(f'{i}) ** ', all_rows)\n",
    "\n",
    "    co_col = col_num - 3\n",
    "    so2_col = col_num - 2\n",
    "    pm10_col = col_num - 1\n",
    "\n",
    "    co = np.nanmean([all_rows[k][co_col] for k in range(node_num)])\n",
    "    so2 = np.nanmean([all_rows[k][so2_col] for k in range(node_num)])\n",
    "    pm10 = np.nanmean([all_rows[k][pm10_col] for k in range(node_num)])\n",
    "\n",
    "    \n",
    "    # print(f'{i})', co, so2, pm10)\n",
    "\n",
    "    # fill the null values with 0.7 * average + 0.3 * previous value\n",
    "    for j in range(node_num):\n",
    "        # co2 \n",
    "        if np.isnan(all_rows[j][co_col]):\n",
    "            if np.isnan(co):\n",
    "                new_dfs[j].iloc[i, co_col] = new_dfs[j].iloc[i - 1, co_col]\n",
    "            else:\n",
    "                new_dfs[j].iloc[i, co_col] = 0.7 * co + 0.3 * new_dfs[j].iloc[i - 1, co_col]\n",
    "            \n",
    "            null_counter[j][0] += 1\n",
    "\n",
    "        # so2\n",
    "        if np.isnan(all_rows[j][so2_col]):\n",
    "            if np.isnan(so2):\n",
    "                new_dfs[j].iloc[i, so2_col] = new_dfs[j].iloc[i - 1, so2_col]\n",
    "            else:\n",
    "                new_dfs[j].iloc[i, so2_col] = 0.7 * so2 + 0.3 * new_dfs[j].iloc[i - 1, so2_col]\n",
    "\n",
    "            null_counter[j][1] += 1\n",
    "\n",
    "        # pm10\n",
    "        if np.isnan(all_rows[j][pm10_col]):\n",
    "            if np.isnan(pm10):\n",
    "                new_dfs[j].iloc[i, pm10_col] = new_dfs[j].iloc[i - 1, pm10_col]\n",
    "            else:\n",
    "                new_dfs[j].iloc[i, pm10_col] = 0.7 * pm10 + 0.3 * new_dfs[j].iloc[i - 1, pm10_col]\n",
    "\n",
    "            null_counter[j][2] += 1\n",
    "        \n",
    "    # if i == 3:\n",
    "    #     break\n",
    "    if i % 500 == 0:\n",
    "        print(f'{i} rows done')\n",
    "\n",
    "print(new_dfs[0])\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "for i in range(len(new_dfs)):\n",
    "\n",
    "    new_dfs[i].to_csv(f'Clean/cleaned_data_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) [5, 2, 240]\n",
      "2) [311, 25, 124]\n",
      "3) [61, 47, 68]\n",
      "4) [12, 5, 71]\n",
      "5) [28, 42, 69]\n",
      "6) [319, 74, 110]\n",
      "7) [26, 10, 169]\n",
      "8) [115, 26, 69]\n",
      "9) [106, 36, 145]\n",
      "10) [311, 22, 61]\n",
      "11) [19, 1, 37]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_dfs)):\n",
    "    print(f'{i+1})', null_counter[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date        co       so2       pm10\n",
      "1     1399/01/01  1.287979    1.7617  20.560321\n",
      "2     1399/01/02   0.73536  2.617922  21.318358\n",
      "3     1399/01/03  0.666496  1.939312   19.22494\n",
      "4     1399/01/04  1.087427  2.246175  11.412533\n",
      "5     1399/01/05  1.496053  2.579158  18.692815\n",
      "...          ...       ...       ...        ...\n",
      "1090  1401/12/25  0.991112  3.445038  33.391259\n",
      "1091  1401/12/26  1.007853  3.218236  48.858406\n",
      "1092  1401/12/27  2.292972  4.125232  44.985591\n",
      "1093  1401/12/28   0.91601  3.863412  40.893744\n",
      "1094  1401/12/29  0.733984  3.481937  31.758359\n",
      "\n",
      "[1094 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfs[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
